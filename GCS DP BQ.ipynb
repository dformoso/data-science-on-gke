{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Google Cloud Storage, Dataproc and BigQuery from Jupyter Notebooks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a requirements text file and write to disk locally\n",
    "- This can be embedded in the container image so not to be required in each notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "google-cloud-storage\n",
    "google-cloud-bigquery\n",
    "google-cloud==0.32.0\n",
    "google-api-python-client\n",
    "google-auth==1.4.1\n",
    "google-auth-httplib2==0.0.3\n",
    "google-auth-oauthlib==0.2.0\n",
    "pytz==2018.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages\n",
    "!pip install -qq --upgrade -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries\n",
    "- The notebook relies on a custum library I've built for this demo, the custom library can also be embedded in the docker instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pyspark, csv, custom_gcp as gcp\n",
    "sc = pyspark.SparkContext.getOrCreate()\n",
    "\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "from google.cloud.bigquery import LoadJobConfig\n",
    "from google.cloud.bigquery import SchemaField\n",
    "import googleapiclient.discovery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and Create Sample Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a sample text file and write to disk locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sample_text_file.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile sample_text_file.txt\n",
    "Hello world! dog elephant panther"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a sample pyspark file and write to disk locally.\n",
    "- The script expects a text file in GCS\n",
    "- gs://spark-on-kubs-bucket/sample_text_file.txt doesn't exist yet, we'll upload it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pyspark_sort_gcs.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pyspark_sort_gcs.py\n",
    "import pyspark\n",
    "\n",
    "sc = pyspark.SparkContext.getOrCreate()\n",
    "txt = sc.textFile('gs://data-science-on-kubs-bucket/sample_text_file.txt')\n",
    "txt = txt.first().split()\n",
    "words = sorted(txt)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a sample pyspark file and write to disk locally\n",
    "- The script declares its own string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pyspark_sort.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pyspark_sort.py\n",
    "import pyspark\n",
    "\n",
    "sc = pyspark.SparkContext()\n",
    "rdd = sc.parallelize(['Hello,', 'world!', 'dog', 'elephant', 'panther'])\n",
    "words = sorted(rdd.collect())\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 56\n",
      "drwsrwsr-x 2 jovyan users  4096 Jul 20 19:33  work\n",
      "-rw-r--r-- 1 jovyan users 16160 Oct  7 06:08  custom_gcp.py\n",
      "drwxr-sr-x 2 jovyan users  4096 Oct  7 06:10  __pycache__\n",
      "-rw-r--r-- 1 jovyan users   176 Oct  7 06:18  requirements.txt\n",
      "-rw-r--r-- 1 jovyan users 12535 Oct  7 06:18 'GCS DP BQ.ipynb'\n",
      "-rw-r--r-- 1 jovyan users    33 Oct  7 06:18  sample_text_file.txt\n",
      "-rw-r--r-- 1 jovyan users   189 Oct  7 06:18  pyspark_sort_gcs.py\n",
      "-rw-r--r-- 1 jovyan users   159 Oct  7 06:18  pyspark_sort.py\n"
     ]
    }
   ],
   "source": [
    "!ls -lrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Variables\n",
    "- Note that here we're using the key.json that was embedded in the container at creation time. We can choose to use the user's account instead if we're creating the Jupyter instance programmatically. For the moment, we'll use a more generic service account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = \"/var/secrets/google/key.json\"\n",
    "GCP_PROJECT = 'data-science-on-kubs'\n",
    "GCP_REGION = 'australia-southeast1'\n",
    "GCP_ZONE = 'australia-southeast1-a'\n",
    "GCS_BUCKET = 'data-science-on-kubs-bucket'\n",
    "GCS_OBJECT = 'sample_text_file.txt'\n",
    "DATAPROC_CLUSTER = 'dataproc-temp-cluster'\n",
    "BIGQUERY_DS_ID = 'bq_test_dataset'\n",
    "BIGQUERY_TABLENAME = 'bq_test_table'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket <Bucket: data-science-on-kubs-bucket> deleted\n",
      "Bucket data-science-on-kubs-bucket created\n",
      "Bucket data-science-on-kubs-bucket found\n",
      "File sample_text_file.txt uploaded to sample_text_file.txt\n",
      "File sample_text_file.txt found\n",
      "Blob sample_text_file.txt downloaded to sample_text_file.txt\n",
      "Blob sample_text_file.txt deleted\n",
      "Bucket data-science-on-kubs-bucket deleted\n"
     ]
    }
   ],
   "source": [
    "# WARNING: THIS DELETES ALL BUCKETS AND OBJECTS IN A PROJECT\n",
    "# USE ONLY ON A TEST, NEW PROJECT\n",
    "gcp.delete_all_buckets_and_blobs(GCP_PROJECT)\n",
    "\n",
    "# Shows how to create, list, delete and upload objects into buckets, as well as deleting objects\n",
    "gcp.create_bucket(GCS_BUCKET, GCP_PROJECT)\n",
    "gcp.list_buckets(GCP_PROJECT)\n",
    "gcp.upload_blob(GCS_BUCKET, GCS_OBJECT, GCS_OBJECT, GCP_PROJECT)\n",
    "gcp.list_blobs(GCS_BUCKET, GCP_PROJECT)\n",
    "gcp.download_blob(GCS_BUCKET, GCS_OBJECT, GCS_OBJECT, GCP_PROJECT)\n",
    "gcp.delete_blob(GCS_BUCKET, GCS_OBJECT, GCP_PROJECT)\n",
    "gcp.delete_bucket(GCS_BUCKET, GCP_PROJECT)\n",
    "gcp.list_buckets(GCP_PROJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket data-science-on-kubs-bucket created\n",
      "Bucket data-science-on-kubs-bucket found\n",
      "File sample_text_file.txt uploaded to sample_text_file.txt\n",
      "File sample_text_file.txt found\n",
      "Blob <Blob: data-science-on-kubs-bucket, sample_text_file.txt> deleted\n",
      "Bucket <Bucket: data-science-on-kubs-bucket> deleted\n"
     ]
    }
   ],
   "source": [
    "# Create bucket and load sample_text_file.txt to it\n",
    "gcp.create_bucket(GCS_BUCKET, GCP_PROJECT)\n",
    "gcp.list_buckets(GCP_PROJECT)\n",
    "gcp.upload_blob(GCS_BUCKET, 'sample_text_file.txt', 'sample_text_file.txt', GCP_PROJECT)\n",
    "gcp.list_blobs(GCS_BUCKET, GCP_PROJECT)\n",
    "gcp.delete_all_buckets_and_blobs(GCP_PROJECT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', 'dog', 'elephant', 'panther', 'world!']\n"
     ]
    }
   ],
   "source": [
    "# PySpark can be run locally\n",
    "rdd = sc.parallelize(['Hello,', 'world!', 'dog', 'elephant', 'panther'])\n",
    "words = sorted(rdd.collect())\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no Dataproc Clusters in this Project and Region\n"
     ]
    }
   ],
   "source": [
    "gcp.list_clusters_with_details(project=GCP_PROJECT, region=GCP_REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket data-science-on-kubs-bucket created\n"
     ]
    }
   ],
   "source": [
    "# WARNING: THIS DELETES ALL BUCKETS AND OBJECTS IN A PROJECT\n",
    "# USE ONLY ON A TEST, NEW PROJECT\n",
    "gcp.delete_all_buckets_and_blobs(GCP_PROJECT)\n",
    "gcp.create_bucket(GCS_BUCKET, GCP_PROJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating cluster...\n",
      "Waiting for cluster creation...\n",
      "Cluster created.\n",
      "dataproc-temp-cluster - RUNNING\n",
      "Submitted job ID 7847c497-e79e-456e-96cf-59ee5c64770f\n",
      "Waiting for job to finish...\n",
      "Job finished.\n",
      "Received job output b\"18/10/07 06:22:22 INFO org.spark_project.jetty.util.log: Logging initialized @8696ms\\n18/10/07 06:22:22 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT\\n18/10/07 06:22:22 INFO org.spark_project.jetty.server.Server: Started @8996ms\\n18/10/07 06:22:22 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@6c26ac19{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\\n18/10/07 06:22:23 INFO com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase: GHFS version: 1.6.10-hadoop2\\n18/10/07 06:22:25 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at dataproc-temp-cluster-m/10.152.0.6:8032\\n18/10/07 06:22:29 WARN org.apache.hadoop.hdfs.DataStreamer: Caught exception\\njava.lang.InterruptedException\\n\\tat java.lang.Object.wait(Native Method)\\n\\tat java.lang.Thread.join(Thread.java:1252)\\n\\tat java.lang.Thread.join(Thread.java:1326)\\n\\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:973)\\n\\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:624)\\n\\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:801)\\n18/10/07 06:22:29 WARN org.apache.hadoop.hdfs.DataStreamer: Caught exception\\njava.lang.InterruptedException\\n\\tat java.lang.Object.wait(Native Method)\\n\\tat java.lang.Thread.join(Thread.java:1252)\\n\\tat java.lang.Thread.join(Thread.java:1326)\\n\\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:973)\\n\\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:624)\\n\\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:801)\\n18/10/07 06:22:30 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1538893212826_0001\\n\\r[Stage 0:>                                                          (0 + 0) / 2]\\r[Stage 0:>                                                          (0 + 1) / 2]\\r[Stage 0:>                                                          (0 + 2) / 2]\\r[Stage 0:=============================>                             (1 + 1) / 2]\\r                                                                                \\r['Hello,', 'dog', 'elephant', 'panther', 'world!']\\n18/10/07 06:22:57 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@6c26ac19{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\\n\"\n",
      "Tearing down cluster\n"
     ]
    }
   ],
   "source": [
    "# USING LOCAL PYSPARK (submit_pyspark_job_to_cluster uploads to GCS first), WITH EMBEDDED STRING\n",
    "# Submit Job to Dataproc Cluster\n",
    "# Note: More parameters for programatic job run can be configured,\n",
    "# check here: https://cloud.google.com/dataproc/docs/reference/rest/v1/ClusterConfig\n",
    "output = gcp.submit_pyspark_job_to_cluster(\n",
    "    project_id=GCP_PROJECT,\n",
    "    zone=GCP_ZONE, \n",
    "    cluster_name=DATAPROC_CLUSTER, \n",
    "    bucket_name=GCS_BUCKET, \n",
    "    pyspark_file='pyspark_sort.py', \n",
    "    create_new_cluster=True,\n",
    "    master_type='n1-standard-1', \n",
    "    worker_type='n1-standard-1',\n",
    "    sec_worker_type='n1-standard-1',\n",
    "    no_masters=1,\n",
    "    no_workers=2, \n",
    "    no_sec_workers=1, \n",
    "    sec_worker_preemptible=True, \n",
    "    dataproc_version='1.2'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blob <Blob: data-science-on-kubs-bucket, pyspark_sort.py> deleted\n",
      "Bucket <Bucket: data-science-on-kubs-bucket> deleted\n",
      "Blob <Blob: dataproc-f56270e1-9774-45c5-98b6-8c9c32b30087-au-southeast1, google-cloud-dataproc-metainfo/62026cd0-822d-45e4-b272-f2c88b2b0571/cluster.properties> deleted\n",
      "Blob <Blob: dataproc-f56270e1-9774-45c5-98b6-8c9c32b30087-au-southeast1, google-cloud-dataproc-metainfo/62026cd0-822d-45e4-b272-f2c88b2b0571/dataproc-temp-cluster-m/dataproc-startup-script_output> deleted\n",
      "Blob <Blob: dataproc-f56270e1-9774-45c5-98b6-8c9c32b30087-au-southeast1, google-cloud-dataproc-metainfo/62026cd0-822d-45e4-b272-f2c88b2b0571/dataproc-temp-cluster-sw-z28c/dataproc-startup-script_output> deleted\n",
      "Blob <Blob: dataproc-f56270e1-9774-45c5-98b6-8c9c32b30087-au-southeast1, google-cloud-dataproc-metainfo/62026cd0-822d-45e4-b272-f2c88b2b0571/dataproc-temp-cluster-w-0/dataproc-startup-script_output> deleted\n",
      "Blob <Blob: dataproc-f56270e1-9774-45c5-98b6-8c9c32b30087-au-southeast1, google-cloud-dataproc-metainfo/62026cd0-822d-45e4-b272-f2c88b2b0571/dataproc-temp-cluster-w-1/dataproc-startup-script_output> deleted\n",
      "Blob <Blob: dataproc-f56270e1-9774-45c5-98b6-8c9c32b30087-au-southeast1, google-cloud-dataproc-metainfo/62026cd0-822d-45e4-b272-f2c88b2b0571/jobs/7847c497-e79e-456e-96cf-59ee5c64770f/driveroutput.000000000> deleted\n",
      "Blob <Blob: dataproc-f56270e1-9774-45c5-98b6-8c9c32b30087-au-southeast1, google-cloud-dataproc-metainfo/62026cd0-822d-45e4-b272-f2c88b2b0571/jobs/7847c497-e79e-456e-96cf-59ee5c64770f/driveroutput.000000001> deleted\n",
      "Bucket <Bucket: dataproc-f56270e1-9774-45c5-98b6-8c9c32b30087-au-southeast1> deleted\n",
      "Bucket data-science-on-kubs-bucket created\n",
      "Bucket data-science-on-kubs-bucket found\n",
      "File sample_text_file.txt uploaded to sample_text_file.txt\n",
      "File sample_text_file.txt found\n"
     ]
    }
   ],
   "source": [
    "# WARNING: THIS DELETES ALL BUCKETS AND OBJECTS IN A PROJECT\n",
    "# USE ONLY ON A TEST, NEW PROJECT\n",
    "gcp.delete_all_buckets_and_blobs(GCP_PROJECT)\n",
    "gcp.create_bucket(GCS_BUCKET, GCP_PROJECT)\n",
    "gcp.list_buckets(GCP_PROJECT)\n",
    "gcp.upload_blob(GCS_BUCKET, 'sample_text_file.txt', 'sample_text_file.txt', GCP_PROJECT)\n",
    "gcp.list_blobs(GCS_BUCKET, GCP_PROJECT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *WARNING*: Your previous Dataproc cluster is terminating, and if you attempt to create a new one before the previous cluster has been torn down, you might reach the default resource limit for this project and have this fail. If so, simply attempt again in a few minutes or increase your limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating cluster...\n",
      "Waiting for cluster creation...\n",
      "Cluster created.\n",
      "dataproc-temp-cluster - RUNNING\n",
      "Submitted job ID 4e87cfcf-b00c-4c8b-8757-c45206b41201\n",
      "Waiting for job to finish...\n",
      "Job finished.\n",
      "Received job output b\"18/10/07 06:27:57 INFO org.spark_project.jetty.util.log: Logging initialized @7579ms\\n18/10/07 06:27:57 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT\\n18/10/07 06:27:57 INFO org.spark_project.jetty.server.Server: Started @7805ms\\n18/10/07 06:27:57 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@23aead7c{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\\n18/10/07 06:27:58 INFO com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase: GHFS version: 1.6.10-hadoop2\\n18/10/07 06:28:00 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at dataproc-temp-cluster-m/10.152.0.5:8032\\n18/10/07 06:28:05 WARN org.apache.hadoop.hdfs.DataStreamer: Caught exception\\njava.lang.InterruptedException\\n\\tat java.lang.Object.wait(Native Method)\\n\\tat java.lang.Thread.join(Thread.java:1252)\\n\\tat java.lang.Thread.join(Thread.java:1326)\\n\\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:973)\\n\\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:624)\\n\\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:801)\\n18/10/07 06:28:07 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1538893560543_0001\\n18/10/07 06:28:26 INFO org.apache.hadoop.mapred.FileInputFormat: Total input files to process : 1\\n[u'Hello', u'dog', u'elephant', u'panther', u'world!']\\n18/10/07 06:28:36 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@23aead7c{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\\n\"\n",
      "Tearing down cluster\n"
     ]
    }
   ],
   "source": [
    "# USING LOCAL PYSPARK (submit_pyspark_job_to_cluster uploads to GCS first), WITH GCS STRING\n",
    "# Submit Job to Dataproc Cluster\n",
    "# Note: More parameters for programatic job run can be configured,\n",
    "# check here: https://cloud.google.com/dataproc/docs/reference/rest/v1/ClusterConfig\n",
    "output = gcp.submit_pyspark_job_to_cluster(\n",
    "    project_id=GCP_PROJECT,\n",
    "    zone=GCP_ZONE, \n",
    "    cluster_name=DATAPROC_CLUSTER, \n",
    "    bucket_name=GCS_BUCKET, \n",
    "    pyspark_file='pyspark_sort_gcs.py', \n",
    "    create_new_cluster=True,\n",
    "    master_type='n1-standard-1', \n",
    "    worker_type='n1-standard-1',\n",
    "    sec_worker_type='n1-standard-1',\n",
    "    no_masters=1,\n",
    "    no_workers=2, \n",
    "    no_sec_workers=1, \n",
    "    sec_worker_preemptible=True, \n",
    "    dataproc_version='1.2'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data-science-on-kubs project does not contain any datasets\n",
      "Dataset bq_test_dataset does not exist\n",
      "Dataset bq_test_dataset created\n",
      "Datasets in project data-science-on-kubs:\n",
      "\tbq_test_dataset\n"
     ]
    }
   ],
   "source": [
    "# List, Create, and Delete Datasets\n",
    "gcp.list_datasets(GCP_PROJECT)\n",
    "gcp.delete_dataset(GCP_PROJECT, BIGQUERY_DS_ID)\n",
    "gcp.create_dataset(GCP_PROJECT, BIGQUERY_DS_ID)\n",
    "gcp.list_datasets(GCP_PROJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bq_test_dataset dataset does not contain any tables\n",
      "Table bq_test_table created\n",
      "Table bq_test_dataset:bq_test_table deleted\n",
      "bq_test_dataset dataset does not contain any tables\n",
      "Table bq_test_table created\n",
      "[SchemaField('full_name', 'STRING', 'REQUIRED', None, ()), SchemaField('age', 'INTEGER', 'REQUIRED', None, ())]\n",
      "None\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# List, Create, and Delete Tables, within a Dataset\n",
    "gcp.list_tables(GCP_PROJECT, BIGQUERY_DS_ID)\n",
    "gcp.create_table(GCP_PROJECT, BIGQUERY_DS_ID, BIGQUERY_TABLENAME)\n",
    "gcp.delete_table(GCP_PROJECT, BIGQUERY_DS_ID, BIGQUERY_TABLENAME)\n",
    "gcp.list_tables(GCP_PROJECT, BIGQUERY_DS_ID)\n",
    "gcp.create_table(GCP_PROJECT, BIGQUERY_DS_ID, BIGQUERY_TABLENAME)\n",
    "gcp.get_table(GCP_PROJECT, BIGQUERY_DS_ID, BIGQUERY_TABLENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert rows into a Table\n",
    "rows_to_insert = [\n",
    "    (u'Phred Phlyntstone', 32),\n",
    "    (u'Wylma Phlyntstone', 29),\n",
    "]  \n",
    "gcp.insert_in_table(GCP_PROJECT, BIGQUERY_TABLENAME, BIGQUERY_DS_ID, rows_to_insert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(('Phred Phlyntstone', 32), {'full_name': 0, 'age': 1})\n",
      "Row(('Wylma Phlyntstone', 29), {'full_name': 0, 'age': 1})\n"
     ]
    }
   ],
   "source": [
    "# Query a Table via SQL\n",
    "table = '`' + GCP_PROJECT +'.'+ BIGQUERY_DS_ID +'.'+ BIGQUERY_TABLENAME + '`'\n",
    "query = 'SELECT * FROM ' + table\n",
    "gcp.query_table(GCP_PROJECT, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blob <Blob: data-science-on-kubs-bucket, pyspark_sort_gcs.py> deleted\n",
      "Blob <Blob: data-science-on-kubs-bucket, sample_text_file.txt> deleted\n",
      "Bucket <Bucket: data-science-on-kubs-bucket> deleted\n",
      "Blob <Blob: dataproc-f56270e1-9774-45c5-98b6-8c9c32b30087-au-southeast1, google-cloud-dataproc-metainfo/de2893ef-51f4-4e97-a451-ce0ca364e104/cluster.properties> deleted\n",
      "Blob <Blob: dataproc-f56270e1-9774-45c5-98b6-8c9c32b30087-au-southeast1, google-cloud-dataproc-metainfo/de2893ef-51f4-4e97-a451-ce0ca364e104/dataproc-temp-cluster-m/dataproc-startup-script_output> deleted\n",
      "Blob <Blob: dataproc-f56270e1-9774-45c5-98b6-8c9c32b30087-au-southeast1, google-cloud-dataproc-metainfo/de2893ef-51f4-4e97-a451-ce0ca364e104/dataproc-temp-cluster-sw-76mj/dataproc-startup-script_output> deleted\n",
      "Blob <Blob: dataproc-f56270e1-9774-45c5-98b6-8c9c32b30087-au-southeast1, google-cloud-dataproc-metainfo/de2893ef-51f4-4e97-a451-ce0ca364e104/dataproc-temp-cluster-w-0/dataproc-startup-script_output> deleted\n",
      "Blob <Blob: dataproc-f56270e1-9774-45c5-98b6-8c9c32b30087-au-southeast1, google-cloud-dataproc-metainfo/de2893ef-51f4-4e97-a451-ce0ca364e104/dataproc-temp-cluster-w-1/dataproc-startup-script_output> deleted\n",
      "Blob <Blob: dataproc-f56270e1-9774-45c5-98b6-8c9c32b30087-au-southeast1, google-cloud-dataproc-metainfo/de2893ef-51f4-4e97-a451-ce0ca364e104/jobs/4e87cfcf-b00c-4c8b-8757-c45206b41201/driveroutput.000000000> deleted\n",
      "Blob <Blob: dataproc-f56270e1-9774-45c5-98b6-8c9c32b30087-au-southeast1, google-cloud-dataproc-metainfo/de2893ef-51f4-4e97-a451-ce0ca364e104/jobs/4e87cfcf-b00c-4c8b-8757-c45206b41201/driveroutput.000000001> deleted\n",
      "Bucket <Bucket: dataproc-f56270e1-9774-45c5-98b6-8c9c32b30087-au-southeast1> deleted\n",
      "Bucket data-science-on-kubs-bucket created\n",
      "Bucket data-science-on-kubs-bucket found\n",
      "Exported data-science-on-kubs:bq_test_dataset.bq_test_table to gs://data-science-on-kubs-bucket/output.csv\n",
      "File output.csv found\n",
      "Blob <Blob: data-science-on-kubs-bucket, output.csv> deleted\n",
      "Bucket <Bucket: data-science-on-kubs-bucket> deleted\n",
      "Table bq_test_dataset:bq_test_table deleted\n"
     ]
    }
   ],
   "source": [
    "# Extract a Table to Google Cloud Storage\n",
    "gcp.delete_all_buckets_and_blobs(GCP_PROJECT)\n",
    "gcp.create_bucket(GCS_BUCKET, GCP_PROJECT)\n",
    "gcp.list_buckets(GCP_PROJECT)\n",
    "destination_uri = 'gs://{}/{}'.format(GCS_BUCKET, 'output.csv')\n",
    "gcp.extract_table_to_gcs(GCP_PROJECT, BIGQUERY_DS_ID, BIGQUERY_TABLENAME, destination_uri)\n",
    "gcp.list_blobs(GCS_BUCKET, GCP_PROJECT)\n",
    "gcp.delete_all_buckets_and_blobs(GCP_PROJECT)\n",
    "gcp.delete_table(GCP_PROJECT, BIGQUERY_DS_ID, BIGQUERY_TABLENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting job 3567c0c9-c4cf-410f-a5d3-5c6c4b3f4309\n",
      "Job finished\n",
      "Loaded 50 rows\n",
      "Row(('Alabama', 'AL'), {'name': 0, 'post_abbr': 1})\n",
      "Row(('Alaska', 'AK'), {'name': 0, 'post_abbr': 1})\n",
      "Row(('Arizona', 'AZ'), {'name': 0, 'post_abbr': 1})\n",
      "Row(('Arkansas', 'AR'), {'name': 0, 'post_abbr': 1})\n",
      "Row(('California', 'CA'), {'name': 0, 'post_abbr': 1})\n",
      "Row(('Colorado', 'CO'), {'name': 0, 'post_abbr': 1})\n",
      "Row(('Connecticut', 'CT'), {'name': 0, 'post_abbr': 1})\n",
      "Row(('Delaware', 'DE'), {'name': 0, 'post_abbr': 1})\n",
      "Row(('Florida', 'FL'), {'name': 0, 'post_abbr': 1})\n",
      "Row(('Georgia', 'GA'), {'name': 0, 'post_abbr': 1})\n",
      "Row(('Hawaii', 'HI'), {'name': 0, 'post_abbr': 1})\n",
      "Row(('Idaho', 'ID'), {'name': 0, 'post_abbr': 1})\n",
      "Row(('Illinois', 'IL'), {'name': 0, 'post_abbr': 1})\n",
      "Row(('Indiana', 'IN'), {'name': 0, 'post_abbr': 1})\n",
      "Row(('Iowa', 'IA'), {'name': 0, 'post_abbr': 1})\n",
      "Row(('Kansas', 'KS'), {'name': 0, 'post_abbr': 1})\n",
      "Row(('Kentucky', 'KY'), {'name': 0, 'post_abbr': 1})\n",
      "Row(('Louisiana', 'LA'), {'name': 0, 'post_abbr': 1})\n",
      "Row(('Maine', 'ME'), {'name': 0, 'post_abbr': 1})\n",
      "Row(('Maryland', 'MD'), {'name': 0, 'post_abbr': 1})\n",
      "Row(('Massachusetts', 'MA'), {'name': 0, 'post_abbr': 1})\n",
      "Row(('Michigan', 'MI'), {'name': 0, 'post_abbr': 1})\n",
      "Row(('Minnesota', 'MN'), {'name': 0, 'post_abbr': 1})\n",
      "Row(('Mississippi', 'MS'), {'name': 0, 'post_abbr': 1})\n",
      "Row(('Missouri', 'MO'), {'name': 0, 'post_abbr': 1})\n",
      "Row(('Montana', 'MT'), {'name': 0, 'post_abbr': 1})\n",
      "Row(('Nebraska', 'NE'), {'name': 0, 'post_abbr': 1})\n",
      "Row(('Nevada', 'NV'), {'name': 0, 'post_abbr': 1})\n",
      "Row(('New Hampshire', 'NH'), {'name': 0, 'post_abbr': 1})\n",
      "Row(('New Jersey', 'NJ'), {'name': 0, 'post_abbr': 1})\n",
      "Row(('New Mexico', 'NM'), {'name': 0, 'post_abbr': 1})\n",
      "Row(('New York', 'NY'), {'name': 0, 'post_abbr': 1})\n",
      "Row(('North Carolina', 'NC'), {'name': 0, 'post_abbr': 1})\n",
      "Row(('North Dakota', 'ND'), {'name': 0, 'post_abbr': 1})\n",
      "Row(('Ohio', 'OH'), {'name': 0, 'post_abbr': 1})\n",
      "Row(('Oklahoma', 'OK'), {'name': 0, 'post_abbr': 1})\n",
      "Row(('Oregon', 'OR'), {'name': 0, 'post_abbr': 1})\n",
      "Row(('Pennsylvania', 'PA'), {'name': 0, 'post_abbr': 1})\n",
      "Row(('Rhode Island', 'RI'), {'name': 0, 'post_abbr': 1})\n",
      "Row(('South Carolina', 'SC'), {'name': 0, 'post_abbr': 1})\n",
      "Row(('South Dakota', 'SD'), {'name': 0, 'post_abbr': 1})\n",
      "Row(('Tennessee', 'TN'), {'name': 0, 'post_abbr': 1})\n",
      "Row(('Texas', 'TX'), {'name': 0, 'post_abbr': 1})\n",
      "Row(('Utah', 'UT'), {'name': 0, 'post_abbr': 1})\n",
      "Row(('Vermont', 'VT'), {'name': 0, 'post_abbr': 1})\n",
      "Row(('Virginia', 'VA'), {'name': 0, 'post_abbr': 1})\n",
      "Row(('Washington', 'WA'), {'name': 0, 'post_abbr': 1})\n",
      "Row(('West Virginia', 'WV'), {'name': 0, 'post_abbr': 1})\n",
      "Row(('Wisconsin', 'WI'), {'name': 0, 'post_abbr': 1})\n",
      "Row(('Wyoming', 'WY'), {'name': 0, 'post_abbr': 1})\n"
     ]
    }
   ],
   "source": [
    "# Load parque file from GCS to BigQuery\n",
    "uri = 'gs://cloud-samples-data/bigquery/us-states/us-states.parquet'\n",
    "gcp.load_gcs_parquet_to_table(GCP_PROJECT, BIGQUERY_TABLENAME, BIGQUERY_DS_ID, uri)\n",
    "table = '`' + GCP_PROJECT +'.'+ BIGQUERY_DS_ID +'.'+ BIGQUERY_TABLENAME + '`'\n",
    "query = 'SELECT * FROM ' + table\n",
    "gcp.query_table(GCP_PROJECT, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset bq_test_dataset deleted\n"
     ]
    }
   ],
   "source": [
    "gcp.delete_all_buckets_and_blobs(GCP_PROJECT)\n",
    "gcp.delete_dataset(GCP_PROJECT, BIGQUERY_DS_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
